{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import Doc_import\n",
    "import os\n",
    "import json\n",
    "import Data_Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_features, train_labels, test_features, test_labels = Doc_import.import_pan21(base_path=r\"C:\\Users\\jan-p\\Documents\\Uni Wuppertal\\Semester 2\\Information \"\n",
    "                                 r\"Retrival\\Projekt\\Data\\pan21\", returns= 100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "100"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocessing_task1_binary(labels):\n",
    "    new_labels = list()\n",
    "    binary = list()\n",
    "    count_ja = 0\n",
    "    count_nein = 0\n",
    "    for label in labels:\n",
    "        if label['multi-author'] == 1:\n",
    "            binary.append(1)\n",
    "            new_labels.append([1,0])\n",
    "            count_ja += 1\n",
    "        else:\n",
    "            binary.append(0)\n",
    "            new_labels.append([0,1])\n",
    "            count_nein += 1\n",
    "    return new_labels,binary, count_ja, count_nein"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 18\n"
     ]
    },
    {
     "data": {
      "text/plain": "[[1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [1, 0],\n [0, 1],\n [1, 0],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [1, 0],\n [1, 0],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [1, 0],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0],\n [0, 1],\n [1, 0],\n [1, 0],\n [1, 0],\n [1, 0]]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels,bin,  ja, nein = preprocessing_task1_binary(train_labels)\n",
    "print(ja, nein)\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'ja = list()\\nnein = list()\\nfor i, b in enumerate(bin):\\n    if b == 1:\\n        ja.append(i)\\n    else: nein.append(i)\\nja = ja[:2800]\\n\\nimport random\\nrandom.shuffle(ja)\\nrandom.shuffle(nein)\\n\\ntrain_labels_new = list()\\ntrain_features_new = list()\\n\\nfor i in range(2800):\\n    train_labels_new.append(labels[nein[i]])\\n    train_labels_new.append(labels[ja[i]])\\n    train_features_new.append(train_features[nein[i]])\\n    train_features_new.append(train_features[ja[i]])\\ntrain_labels_new'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''ja = list()\n",
    "nein = list()\n",
    "for i, b in enumerate(bin):\n",
    "    if b == 1:\n",
    "        ja.append(i)\n",
    "    else: nein.append(i)\n",
    "ja = ja[:2800]\n",
    "\n",
    "import random\n",
    "random.shuffle(ja)\n",
    "random.shuffle(nein)\n",
    "\n",
    "train_labels_new = list()\n",
    "train_features_new = list()\n",
    "\n",
    "for i in range(2800):\n",
    "    train_labels_new.append(labels[nein[i]])\n",
    "    train_labels_new.append(labels[ja[i]])\n",
    "    train_features_new.append(train_features[nein[i]])\n",
    "    train_features_new.append(train_features[ja[i]])\n",
    "train_labels_new'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import Data_Preprocessing\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        kernel = 4\n",
    "        input_ff = int(393216/(kernel * kernel))\n",
    "        zwischenlayer1 = int(1000)\n",
    "        zwischenlayer2 = int(10)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.pooling = nn.MaxPool2d(kernel)\n",
    "        self.linear11 = nn.Linear(input_ff, zwischenlayer1)\n",
    "        self.linear12 = nn.Linear(zwischenlayer1, zwischenlayer2)\n",
    "        self.linear13 = nn.Linear(zwischenlayer2, 2)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        bert_output = self.bert(**input_data)\n",
    "        bert_output = self.pooling(bert_output[0])\n",
    "        bert_output = bert_output[0].view(-1, )\n",
    "        x1 = torch.sigmoid(self.linear11(bert_output))\n",
    "        x1 = torch.sigmoid(self.linear12(x1))\n",
    "        x1 = torch.sigmoid(self.linear13(x1))\n",
    "\n",
    "        return x1\n",
    "\n",
    "    def train_x1(self, features, labels, loss_function, optimizer, epochs):\n",
    "        log_interval = 5\n",
    "        evaluation_list = list()\n",
    "        for epoch in range(epochs):\n",
    "            for i, data in enumerate(features):\n",
    "                data = Data_Preprocessing.preprocessing(data)\n",
    "                optimizer.zero_grad()\n",
    "                output1 = self.forward(data)\n",
    "                target =  torch.tensor(labels[i])\n",
    "                target = target.to(torch.float32)\n",
    "                #print(torch.tensor(output1[0][0]),torch.tensor(float(labels[i]['multi-author'])))\n",
    "                print(output1[0],target)\n",
    "                #loss_x1 = loss_function(torch.tensor(output1[0][0]), torch.tensor(float(labels[i]['multi-author'])))\n",
    "                loss_x1 = loss_function(output1[0],target)\n",
    "                evaluation_list.append(([output1,torch.tensor(labels[i]), loss_x1]))\n",
    "                print(i, '  ist=', output1, ' soll=', torch.tensor(labels[i]), ' Loss=', loss_x1)\n",
    "                loss_x1.backward()\n",
    "                optimizer.step()\n",
    "                #if index % log_interval == 0:\n",
    "                 #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epochs, index * len(features),\n",
    "                  #                                                                 len(features),\n",
    "                   #                                                                100. * index / len(features),\n",
    "                    #                                                               loss_x1.data.item()))\n",
    "        return evaluation_list\n",
    "\n",
    "    def test_model(self, features, labels, loss_function):\n",
    "        log_interval = 5\n",
    "        evaluation_list = list()\n",
    "\n",
    "        for index, data in enumerate(features):\n",
    "            output = self(data)\n",
    "            target =  torch.tensor(labels[index])\n",
    "            target = target.to(torch.float32)\n",
    "            loss = loss_function(output[0], target)\n",
    "            evaluation_list.append(([output,torch.tensor(labels[index]['multi-author']), loss]))\n",
    "\n",
    "\n",
    "\n",
    "        return evaluation_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jan-p\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4549, 0.5590], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([2])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-fd602826929c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mevaluation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_x1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain_features\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlabels\u001B[0m \u001B[1;33m,\u001B[0m\u001B[0mloss_function\u001B[0m\u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m=\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-10-35f97144ed9c>\u001B[0m in \u001B[0;36mtrain_x1\u001B[1;34m(self, features, labels, loss_function, optimizer, epochs)\u001B[0m\n\u001B[0;32m     43\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m                 \u001B[1;31m#loss_x1 = loss_function(torch.tensor(output1[0][0]), torch.tensor(float(labels[i]['multi-author'])))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 45\u001B[1;33m                 \u001B[0mloss_x1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     46\u001B[0m                 \u001B[0mevaluation_list\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0moutput1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_x1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     47\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'  ist='\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m' soll='\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m' Loss='\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_x1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    610\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    611\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 612\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbinary_cross_entropy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreduction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    613\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    614\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mbinary_cross_entropy\u001B[1;34m(input, target, weight, size_average, reduce, reduction)\u001B[0m\n\u001B[0;32m   3054\u001B[0m         \u001B[0mreduction_enum\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_enum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreduction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3055\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3056\u001B[1;33m         raise ValueError(\n\u001B[0m\u001B[0;32m   3057\u001B[0m             \u001B[1;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3058\u001B[0m             \u001B[1;34m\"Please ensure they have the same size.\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "evaluation = model.train_x1(features=train_features,labels=labels ,loss_function= criterion,optimizer= optimizer,epochs= 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
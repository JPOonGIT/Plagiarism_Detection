{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import Doc_import\n",
    "import os\n",
    "import json\n",
    "import Data_Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "train_features, train_labels, test_features, test_labels = Doc_import.import_pan21(base_path=r\"C:\\Users\\jan-p\\Documents\\Uni Wuppertal\\Semester 2\\Information \"\n",
    "                                 r\"Retrival\\Projekt\\Data\\pan21\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[  101,  2004,  3357, 16001,  2669,  2056,  1999,  1996,  7928,  2000,\n          2115,  3160,  1010,  9207,  8785,  2003,  5186,  2590,  2005,  3492,\n          2172,  2151, 14134,  2030,  7605,  2208,  1012,  2174,  1010,  5584,\n          3716,  3475,  1005,  1056,  4072,  2005,  1037,  2843,  1997,  3722,\n          2399,  1012,  2045,  2024,  5584,  1011,  2066,  8474,  2017,  2323,\n          3305,  1037,  2978,  2055,  1010,  2066, 12365,  1010,  2021,  2017,\n          2180,  1005,  1056,  2342, 19276,  2030,  5584,  4280,  2005,  2008,\n          2004,  2146,  2004,  2017,  2562,  2009,  3722,  1012,  1037,  2843,\n          1997,  2477,  2017,  2089,  2215,  2000,  2079,  2064,  2022, 23599,\n          3432,  2438,  2008,  2867,  2180,  1005,  1056,  2729,  2172,  1010,\n          2066, 15012,  1010,  2030,  8058,  1010,  2030,  8992,  1012,  1037,\n         11519, 10616,  1997,  5584,  2097,  3497,  2393,  1999,  2116,  8146,\n          2295,  1012,   102],\n        [  101,  2009,  2003,  2763,  2025,  3223,  2000,  2113,  5584,  1999,\n          4751,  2043,  2017,  1005,  2128,  2725,  1037,  2208,  1010,  2021,\n          2009,  5791,  7126,  1010,  2926,  2065,  2045,  2024,  2070,  1005,\n          7484,  4507,  1005,  2838,  1999,  2115,  2208,  1012,  1037,  2208,\n          2066,  1000,  2013,  6497,  1000,  1006,  4388, 15775,  4048,  1007,\n          2003,  7687,  5584, 12504, 11721,  4328, 10451,  1010,  2096,  1000,\n          2178,  2088,  1000,  2069,  2342,  2152,  1011, 11718,  5425,  1997,\n          2613,  1011,  2166,  4367,  1006,  2061,  1998,  5942,  2210,  2000,\n          2053,  5025,  4824,  1997,  2054,  6433,  1007,  1012,   102,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  2009,  2003,  2200,  3497,  2008,  1999,  8743,  2401,  1998,\n          1996,  2060,  4230,  4367,  3141,  4249,  2097,  2022,  3243, 14044,\n          2000,  3965,  2242,  2008,  2003,  8242,  2000,  1996,  3239,  1010,\n          2130,  2065,  2017,  1005,  2128,  2074,  3048,  5167,  2006,  1037,\n          7433,  6277,  1012,  4209,  2008, 15012,  6526,  7126,  2478,  2009,\n          2004,  1037,  2208, 15893,  1010,  2130,  2065,  2017,  2123,  1005,\n          1056,  3599,  2224,  2037,  3834,  2544,  1012,  2168,  2005,  2943,\n          5680,  1012,  1999,  2755,  1010,  1045,  2903,  2057,  2071,  2200,\n          2172,  2224,  2208,  2458,  2004,  1037,  2126,  2000,  6570,  5584,\n          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  2383,  2242,  2008, 16582,  2015,  2485,  2000,  2256,  2613,\n          2088,  3084,  1996,  3513,  1997,  2115,  2208,  6082,  2000,  4553,\n          2005,  2115,  2447,  1010,  2514,  2062,  3019,  1010,  1998,  7545,\n          1996,  2447,  2000,  1996,  4569,  1013, 10368,  2112,  2062,  5901,\n          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  1996,  2204,  3921,  2003,  2172, 16325,  1998,  3432,  5478,\n          2008,  1996,  3177,  2003,  3445,  2012,  2296,  4853,  2011,  1037,\n          5377,  1006,  8992, 16264,  1007,  2004,  3090,  1999,  8446,  4277,\n          1012,  4209,  5584,  3084,  2017,  2424,  1996,  2157,  5576,  5514,\n          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  5584,  3475,  1005,  1056,  2428,  4072,  4983,  2017,  2215,\n          2000,  2031,  5584,  1999,  2115,  2208,  1012,  2096,  1037,  2204,\n          2236,  3716,  1997,  5584,  2003,  6749,  1010,  2009,  1005,  1055,\n          2025,  4072,  2065,  2017,  1005,  2128,  2478,  2619,  2842,  1005,\n          1055,  5584,  3194,  1006,  2029,  1045,  2052, 16755,  1007,  1012,\n          2036,  1010, 10250,  2278,  2241,  5584,  2097,  2131,  2017,  8736,\n          1012,  1998,  1010,  2017,  2428,  2069,  2342,  2000,  2113,  9760,\n          2021,  1996,  2060,  2477,  2123,  1005,  1056,  3480,  1012,  2035,\n          1999,  2035,  2295,  1010,  2009,  9041,  2006,  2054,  2017,  1005,\n          2128,  2725,  1012,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  1045,  1005,  1040,  2022,  2200,  4527,  2065,  2017,  2412,\n          2224, 10691,  2030, 15313,  8106,  1010,  2295,  1012,  2021,  2040,\n          4282,  1029, 11320,  8566,  2213,  8108,  2071,  6011,  2033,  3308,\n          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  1996,  2034,  2051,  1045,  2359,  2000,  2031,  1037,  2839,\n          8660,  1999,  1037,  2208,  1010,  1045,  2318,  2478,  8254,  1006,\n          1060,  1007,  1012,  1996,  2765,  2001, 17704, 17712,  7652,  2015,\n          1024,  2004,  2574,  2004,  2017,  5598,  2125,  1037,  7656,  1010,\n          2017,  1005,  1040,  2203,  2039,  8932,  2039,  1998,  2091,  1010,\n          2004,  2065,  2017,  1005,  1040,  2022,  5559,  1037,  8025,  7160,\n         11751,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0]])}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  101,  2004,  3357, 16001,  2669,  2056,  1999,  1996,  7928,  2000,\n          2115,  3160,  1010,  9207,  8785,  2003,  5186,  2590,  2005,  3492,\n          2172,  2151, 14134,  2030,  7605,  2208,  1012,  2174,  1010,  5584,\n          3716,  3475,  1005,  1056,  4072,  2005,  1037,  2843,  1997,  3722,\n          2399,  1012,  2045,  2024,  5584,  1011,  2066,  8474,  2017,  2323,\n          3305,  1037,  2978,  2055,  1010,  2066, 12365,  1010,  2021,  2017,\n          2180,  1005,  1056,  2342, 19276,  2030,  5584,  4280,  2005,  2008,\n          2004,  2146,  2004,  2017,  2562,  2009,  3722,  1012,  1037,  2843,\n          1997,  2477,  2017,  2089,  2215,  2000,  2079,  2064,  2022, 23599,\n          3432,  2438,  2008,  2867,  2180,  1005,  1056,  2729,  2172,  1010,\n          2066, 15012,  1010,  2030,  8058,  1010,  2030,  8992,  1012,  1037,\n         11519, 10616,  1997,  5584,  2097,  3497,  2393,  1999,  2116,  8146,\n          2295,  1012,   102],\n        [  101,  2009,  2003,  2763,  2025,  3223,  2000,  2113,  5584,  1999,\n          4751,  2043,  2017,  1005,  2128,  2725,  1037,  2208,  1010,  2021,\n          2009,  5791,  7126,  1010,  2926,  2065,  2045,  2024,  2070,  1005,\n          7484,  4507,  1005,  2838,  1999,  2115,  2208,  1012,  1037,  2208,\n          2066,  1000,  2013,  6497,  1000,  1006,  4388, 15775,  4048,  1007,\n          2003,  7687,  5584, 12504, 11721,  4328, 10451,  1010,  2096,  1000,\n          2178,  2088,  1000,  2069,  2342,  2152,  1011, 11718,  5425,  1997,\n          2613,  1011,  2166,  4367,  1006,  2061,  1998,  5942,  2210,  2000,\n          2053,  5025,  4824,  1997,  2054,  6433,  1007,  1012,   102,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  2009,  2003,  2200,  3497,  2008,  1999,  8743,  2401,  1998,\n          1996,  2060,  4230,  4367,  3141,  4249,  2097,  2022,  3243, 14044,\n          2000,  3965,  2242,  2008,  2003,  8242,  2000,  1996,  3239,  1010,\n          2130,  2065,  2017,  1005,  2128,  2074,  3048,  5167,  2006,  1037,\n          7433,  6277,  1012,  4209,  2008, 15012,  6526,  7126,  2478,  2009,\n          2004,  1037,  2208, 15893,  1010,  2130,  2065,  2017,  2123,  1005,\n          1056,  3599,  2224,  2037,  3834,  2544,  1012,  2168,  2005,  2943,\n          5680,  1012,  1999,  2755,  1010,  1045,  2903,  2057,  2071,  2200,\n          2172,  2224,  2208,  2458,  2004,  1037,  2126,  2000,  6570,  5584,\n          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  2383,  2242,  2008, 16582,  2015,  2485,  2000,  2256,  2613,\n          2088,  3084,  1996,  3513,  1997,  2115,  2208,  6082,  2000,  4553,\n          2005,  2115,  2447,  1010,  2514,  2062,  3019,  1010,  1998,  7545,\n          1996,  2447,  2000,  1996,  4569,  1013, 10368,  2112,  2062,  5901,\n          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  1996,  2204,  3921,  2003,  2172, 16325,  1998,  3432,  5478,\n          2008,  1996,  3177,  2003,  3445,  2012,  2296,  4853,  2011,  1037,\n          5377,  1006,  8992, 16264,  1007,  2004,  3090,  1999,  8446,  4277,\n          1012,  4209,  5584,  3084,  2017,  2424,  1996,  2157,  5576,  5514,\n          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  5584,  3475,  1005,  1056,  2428,  4072,  4983,  2017,  2215,\n          2000,  2031,  5584,  1999,  2115,  2208,  1012,  2096,  1037,  2204,\n          2236,  3716,  1997,  5584,  2003,  6749,  1010,  2009,  1005,  1055,\n          2025,  4072,  2065,  2017,  1005,  2128,  2478,  2619,  2842,  1005,\n          1055,  5584,  3194,  1006,  2029,  1045,  2052, 16755,  1007,  1012,\n          2036,  1010, 10250,  2278,  2241,  5584,  2097,  2131,  2017,  8736,\n          1012,  1998,  1010,  2017,  2428,  2069,  2342,  2000,  2113,  9760,\n          2021,  1996,  2060,  2477,  2123,  1005,  1056,  3480,  1012,  2035,\n          1999,  2035,  2295,  1010,  2009,  9041,  2006,  2054,  2017,  1005,\n          2128,  2725,  1012,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  1045,  1005,  1040,  2022,  2200,  4527,  2065,  2017,  2412,\n          2224, 10691,  2030, 15313,  8106,  1010,  2295,  1012,  2021,  2040,\n          4282,  1029, 11320,  8566,  2213,  8108,  2071,  6011,  2033,  3308,\n          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  1996,  2034,  2051,  1045,  2359,  2000,  2031,  1037,  2839,\n          8660,  1999,  1037,  2208,  1010,  1045,  2318,  2478,  8254,  1006,\n          1060,  1007,  1012,  1996,  2765,  2001, 17704, 17712,  7652,  2015,\n          1024,  2004,  2574,  2004,  2017,  5598,  2125,  1037,  7656,  1010,\n          2017,  1005,  1040,  2203,  2039,  8932,  2039,  1998,  2091,  1010,\n          2004,  2065,  2017,  1005,  1040,  2022,  5559,  1037,  8025,  7160,\n         11751,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_input['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "755712"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0].view(-1,1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import Model_Test\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]['multi-author']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Model_Test.Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "criterion = nn.NLLLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "array([list([\"As stephelton said in the comments to your question, vector math is extremely important for pretty much any 2D or 3D game. However, physics knowledge isn't necessary for a lot of simple games. There are physics-like concepts you should understand a bit about, like collision, but you won't need calculus or physics classes for that as long as you keep it simple. A lot of things you may want to do can be simulated simply enough that players won't care much, like friction, or sliding, or gravity. A decent grasp of physics will likely help in many situations though.\\n\", 'It is probably not required to know physics in details when you\\'re doing a game, but it definitely helps, especially if there are some \\'virtual reality\\' features in your game. A game like \"From Dust\" (Eric Chahi) is essentially physics simulation gamified, while \"Another World\" only need high-precision capture of real-life motion (so and requires little to no actual understanding of what happens).\\n', \"It is very likely that inertia and the other bodies motion related fields will be quite helpful to produce something that is pleasant to the eye, even if you're just moving items on a chessboard. Knowing that friction exists helps using it as a game mechanic, even if you don't exactly use their academic version. Same for energy conservation. In fact, I believe we could very much use game development as a way to teach physics.\\n\", 'Having something that behaves close to our real world makes the rules of your game easier to learn for your player, feel more natural, and brings the player to the fun/challenging part more rapidly.\\n', 'The good approach is much simpler and simply require that the speed is increased at every frame by a constant (gravity acceleration) as stated in Newton Laws. Knowing physics makes you find the right solution faster.\\n', \"Physics isn't really necessary unless you want to have physics in your game. While a good general knowledge of physics is recommended, it's not necessary if you're using someone else's physics engine (which I would recommend). Also, calc based physics will get you farther. And, you really only need to know mechanics but the other things don't hurt. All in all though, it depends on what you're doing.\\n\", \"I'd be very surprised if you ever use Maxwell or Einstein theories, though. But who knows ? Ludum dare could prove me wrong.\\n\", \"The first time I wanted to have a character jumping in a game, I started using sin(x). The result was kinda akwards: as soon as you jumped off a cliff, you'd end up travelling up and down, as if you'd be riding a curious deltaplane.\"]),\n       list(['What RFC 4862 states about prefix lifetimes has nothing to do with SLAAC, this is pure about how long on-link prefixes are valid to perform neighbor discovery.\\n', 'For prefix assignment using SLAAC (RFC 4862), the lifetimes are fully governed by the description in section 5.5.3. Router Advertisement Processing. To simplify:\\n', \"So in short, when it comes to deprecation you can make sure that a SLAAC address is no longer used for new connections (set preferred lifetime to 0) but you cannot kill ongoing sessions as long as the valid lifetime is running. Of course various implementations might provide administrative interfaces to forcefully remove an address, but that's outside of RFC scope.\\n\", 'However, prefix is not forgotten immediately by host, at least if the implementation is robust and follows RFC4862. There are additional rules for prefix lifetime handling:\\n', 'As far as the RFC goes, you can put the preferred lifetime to anything, though 0 does not make sense. The valid lifetime cannot be 0 except when deprecating. Preferred must always be bigger than valid. So what time you put it is your own choice and depends on your use case.']),\n       list(['Another potential problem with mail is the reverse dns setting. Sometimes, the destination servers check who you claim to be by checking your source ip against the domain that you claim to represent. You may need to set this up with your DNS provider.\\n', \"In any event make sure to enable logging in the properties of the IIS SMTP server and then check the logs after you receive an NDR. If there are no entries in the log file that correspond to the email that generated the NDR then I would suspect a problem with the IIS server's dns client being able to resolve the MX record for the recipient's domain. If there are entries in the log file that correspond to the email that generated the NDR, then the SMTP status codes in the log file should clue you in to what's happening.\\n\", 'You should first check that you can actually connect to the destination SMTP mail server from your internal server. You can do that by telnet <destination server> 25 and see if you get the greeting. Sometimes, some places filter out port 25 connections as a spam prevention policy.\\n', 'The number of users that report you as spam and the number of bad email addresses you send to will also affect your ability to send.\\n', \"You'll also be better off if you submit the IP(s) to the various whitelist / bulk sender lists, such as AOL and yahoo.\\n\", 'Some mailservers will limit your connections until you establish that IP address as a safe sender.  This takes several things - you have to adhere to best practives for settings up:\\n', \"Where is the NDR coming from, presumably your IIS SMTP server, although I don't work with IIS SMTP very often so I'm not sure if the IIS SMTP server is capable of generating NDR's.\\n\", \"Just because your application isn't sending spam doesn't mean that another app on the shared host isnt. Since it's a shared host, all smtp traffic comes from the same source, which can end up being blocked.\\n\", 'Finally, if you are sending emails from users that aren\\'t on your domain, you\\'re likely to always have some issues.  You might try sending from a specific email address, and just changing the from name and \"reply-to\" address in the email.\\n', \"Who are you hosting your application with? I've had problems with shared hosts having all sites hosted on their servers blacklisted into the spam hole.\"]),\n       ...,\n       list(['So I thought, what about making the accuracy of your assessment part of the whole game. Like the trainer has stats which determine the accuracy of what you see as your champions stats. At first you might only be able to judge strength, at least perhaps until you train a champion that is, say, highly intelligent so raising it allows you to judge Intelligence as a value.\\n', 'If a player can neither see nor influence a stat then what is the point of making it variable? As I see it you get the full effect of the downside, but none of the upsides.\\n', 'The downside of variable stats is that you need to design the game to be enjoyable no matter how the stats have developed. This can be quite hard as tuning a fight to the desirable difficulty for a player with one stat set will often make it either too hard or too easy for a player with a different stat set.\\n', \"The problem I foresee is having random stats. While it sounds good on paper, it can yield very different results. Gamers have different play styles and if you force them out of what they are comfortable with, they can get frustrated. They may even resort to restarting the game multiple times when they discover their character isn't how they want it to be.\\n\", 'If the player just stops working that is going to be very frustrating to the player. You need to communicate the mana levels somehow. You can do this by the character visibly changing or complaining more vehemently as the mana levels decrease.\\n', 'Hidden stats can be fun if done correctly. It sounds like you are going in an interesting and unique direction which can land you in a very good place.\\n', 'That way you can still have hidden stats but the player still has some control over how the character grows.\\n', \"So, while I think that it could be used in some really cool ways in your poke-champion style RPG, my general view on the matter would be that if you are making an RPG, include the stats. If you feel the numbers detract from the experience and want to make the character's status apparent through how he looks or how the control changes YOU ARE MOVING AWAY FROM THE TRADITIONAL RPG, which is fine however if you are making a real RPG which utilizes a stats system, you should at least give the player a screen where he is given the numerical values for his abilities and attributes. As the guy above me said (paraphrased) how exactly does your game BENEFIT from what you want to do differently- it is not enough to want to hide the stats simply because it would be cool to hide the stats. Also, it was said that giving people more information allows them to make more informed decisions... giving them TOO MUCH information- either by drowning them in too much data or by giving them unintentionally misleading or unclear data, is just as bad. One must strike a narrow balance, which is why the traditional style is so commonly used-  it manages to strike that balance and people are already familiar with it. \\n\", \"This too has to be communicated or it is going to be really hard for the player to notice. Say if a character has a propensity for archery that should be built into their back story or they should volunteer or show enthusiasm for tasks they are talented at. So I think just a random talent set is a bad idea unless you can communicate the talents in some other way than observation or the player's trial and error.\\n\", 'With regards to your \"pokemoon-esque\" game idea- having your OWN champions\\' stats be hidden from the player at first seemed a BAD idea, or rather one that I didn\\'t like at first... however, I HAVE always felt that games which were attempting to be as \"realistic\" as possible would ultimately not even want to use stats, per se. If your pokemon game were \"real life\", one would not have the convenience of having a numerical value for, say \"strength\" but rather one would have to judge its strength based upon one\\'s own knowledge and familiarity with the thing. Perhaps someone who is an experienced trainer can end up with a result quite similar to a numerical value, but a novice surely wouldn\\'t.\\n', \"Because you are able to judge it doesn't mean that you are able to judge it well, or even accurately so you would perhaps have a stat which corresponds to the likelihood of the judgement being inaccurate, as well as one for the severity of the inaccuracy.\\n\", 'I think there is one very important aspect you have to deliver on and if you can then this is a perfectly good idea. And that is communication. The problem with abstracting over the numbers is that the difference may be very difficult or observe due to the manifesting over long periods of time or being very subtle. The number are an obvious if very crude way to communicative to the user exactly what is happening. \\n', 'Maybe instead of random stats, you have them answer a quiz or perform actions on a vanilla character and base stats on the answers the player chooses or the actions they perform.\\n', 'The point of having variable stats in the first place is first and foremost that the player can increase those stats. It is a simple but effective skinner box technique. Second it gives the player some customization options, choosing what skill to level up feels like making an important decision, and that is good.\\n', \"I enjoy when there is an entirely new way of doing things with regards to the battle system in an RPG; however while I do enjoy the process of learning a well-designed, unfamiliar system I always end up feeling like I've finally got the hang of it right in time for the last battle... so, it's a balance. Hope this helped :) \"]),\n       list(['Say you have to pick a door to open among 10000 doors (say there is a prize behind one of the doors). Choosing randomly means you would choose one out of the 10000 doors and enter it. If there is a prize behind only one door, you will most probably not find it. A non-deterministic machine would enter all 10000 doors simultaneously. If there is a prize anywhere, the non-deterministic machine will find it.\\n', 'Definition of Non-Deterministic Turing Machine: A Turing machine which has more than one next state  for some combinations of contents of the current cell and current state. An input is accepted if any move sequence leads to acceptance.\\n', 'To keep it simple: a non-deterministic machine can optimally choose the outcome of each coin flip (if you like the analogy with a probabilistic machine). You might also imagine that it executes the computation for each outcome of the coin flip in parallel.\\n', 'There are several different contexts where “deterministic”, “random” and “non-deterministic” mean three different things. In contexts where there are multiple participants, such as security and concurrency, the intuition is often something like:\\n', 'A nondeterministic Turing Machine (TM) which randomly chooses between available transitions at each point according to some probability distribution.']),\n       list([\"I'm not Word user, but I'd guess that when a document is saved with a given encoding, the codepage for the language is stored as part of the document metadata.\\n\", 'both containing Japanese text. If you copy some text from one text file and paste it to other you will see a successful pasting. you must ask that they are different encoding how come they are getting pasted in the same text file?\\n', 'the answer is when you copy some text it get copied in UNICODE and also pasted as unicode(with my little programming knowledge all text that is displayed on windows are in unicode. If you dont convert text to Unicode you cannot print them properly). But when user save the file, it get saved in its proper encoding....................\\n', 'Yes, you need to use an editor that supports multiple encodings. Most full-featured editors (notepad++, (g)vim, etc.) support this. Usually, you need to tell the editor which encoding the text file has (because this information is not in the file itself), though sometimes the editor can guess it for you. E.g. in Notepad++ and gvim there are menu/config settings for choosing the encoding.\\n', 'For plain text files, in general you cannot tell the encoding. However, you can guess (mostly based on byte frequency); this is probably what Word does.\\n', 'But it is very much possible that you can use different language in the same text file. But it must have to be same encoding. \\n', 'http://www.microsoft.com/downloads/details.aspx?familyid=8C4E8E0D-45D1-4D9B-B7C0-8430C1AC89AB&displaylang=en \\n', 'For reading in text editors, see above. As to \"SecureCRT\": I assume you mean the file is on a remote server which you access via SecureCRT. Then you need to use an editor on the remote system that supports the file\\'s encoding.\\n', 'But if you might get some pretty interesting issue where it will seems that you are getting data in different encoding... as example\\n', 'if you have development idea. You can try simple test on encoding issue. try to print two different encoded data in the same page. you will get garbage output.'])],\n      dtype=object)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-55-72695cb9745a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mencoded_input\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mData_Preprocessing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_features\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mencoded_input\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\Plagiarism_Detection\\Data_Preprocessing.py\u001B[0m in \u001B[0;36mpreprocessing\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBertTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'bert-base-uncased'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mencoded_input\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturn_tensors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'pt'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mencoded_input\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2449\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2450\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0m_is_valid_text_input\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2451\u001B[1;33m             raise ValueError(\n\u001B[0m\u001B[0;32m   2452\u001B[0m                 \u001B[1;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2453\u001B[0m                 \u001B[1;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "encoded_input = Data_Preprocessing.preprocessing(train_features)\n",
    "encoded_input"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(encoded_input['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    246\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 247\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    248\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'size'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-47-b47197b31a03>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_x1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoded_input\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_labels\u001B[0m \u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\Plagiarism_Detection\\Model_Test.py\u001B[0m in \u001B[0;36mtrain_x1\u001B[1;34m(self, features, labels, loss_function, optimizer, epochs)\u001B[0m\n\u001B[0;32m     28\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m                 \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 30\u001B[1;33m                 \u001B[0moutput1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     31\u001B[0m                 \u001B[0mloss_x1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'multi-author'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m                 \u001B[0mloss_x1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\Plagiarism_Detection\\Model_Test.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m         \u001B[0mbert_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m         \u001B[0mbert_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbert_output\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[0mx1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear11\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbert_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    963\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    964\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0minput_ids\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 965\u001B[1;33m             \u001B[0minput_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput_ids\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    966\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0minputs_embeds\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    967\u001B[0m             \u001B[0minput_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minputs_embeds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    247\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    248\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 249\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    250\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    251\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getstate__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.train_x1(encoded_input,train_labels , criterion, optimizer, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
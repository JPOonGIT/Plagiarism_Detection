{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import Doc_import\n",
    "import os\n",
    "import json\n",
    "import Data_Preprocessing\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_train_labels = '/Users/alexbeetz/pan20-authorship-verification-training-small/pan20-authorship-verification-training-small-truth.jsonl'\n",
    "fname_train_features = '/Users/alexbeetz/pan20-authorship-verification-training-small/pan20-authorship-verification-training-small.jsonl'\n",
    "\n",
    "same_author, texts = Doc_import.text_preprocessing(fname_train_labels, fname_train_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_task2_binary(same_author):\n",
    "    new_labels = []\n",
    "    binary = []\n",
    "    count_ja = 0\n",
    "    count_nein = 0\n",
    "    values = list(same_author.values())\n",
    "    for label in values:\n",
    "        if label == 1:\n",
    "            binary.append(1)\n",
    "            new_labels.append([1,0])\n",
    "            count_ja += 1\n",
    "        else:\n",
    "            binary.append(0)\n",
    "            new_labels.append([1,0])\n",
    "            count_nein += 1\n",
    "    return new_labels, binary, count_ja, count_nein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, bin, ja, nein = preprocessing_task2_binary(same_author)\n",
    "#print(ja, nein)\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja = list()\n",
    "nein = list()\n",
    "for i, b in enumerate(bin):\n",
    "    if b == 1:\n",
    "        ja.append(i)\n",
    "    else: nein.append(i)\n",
    "ja = ja[:2800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(ja)\n",
    "random.shuffle(nein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_new = list()\n",
    "train_features_new = list()\n",
    "\n",
    "for i in range(2800):\n",
    "    train_labels_new.append(labels[nein[i]])\n",
    "    train_labels_new.append(labels[ja[i]])\n",
    "    train_features_new.append(texts[nein[i]])\n",
    "    train_features_new.append(texts[ja[i]])\n",
    "#train_labels_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import Data_Preprocessing\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        kernel = 4\n",
    "        input_ff = int(393216/(kernel * kernel))\n",
    "        zwischenlayer1 = int(1000)\n",
    "        zwischenlayer2 = int(10)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.pooling = nn.MaxPool2d(kernel)\n",
    "        self.linear11 = nn.Linear(input_ff, zwischenlayer1)\n",
    "        self.linear12 = nn.Linear(zwischenlayer1, zwischenlayer2)\n",
    "        self.linear13 = nn.Linear(zwischenlayer2, 2)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        bert_output = self.bert(**input_data)\n",
    "        bert_output = self.pooling(bert_output[0])\n",
    "        bert_output = bert_output[0].view(-1, 24576)\n",
    "        x1 = nn.functional.sigmoid(self.linear11(bert_output))\n",
    "        x1 = nn.functional.sigmoid(self.linear12(x1))\n",
    "        x1 = nn.functional.sigmoid(self.linear13(x1))\n",
    "\n",
    "        return x1\n",
    "\n",
    "    def train_x1(self, features, labels, loss_function, optimizer, epochs):\n",
    "        log_interval = 5\n",
    "        evaluation_list = list()\n",
    "        for epoch in range(epochs):\n",
    "            for i, data in enumerate(features):\n",
    "                data = Data_Preprocessing.preprocessing(data)\n",
    "                optimizer.zero_grad()\n",
    "                output1 = self.forward(data)\n",
    "                target =  torch.tensor(labels[i])\n",
    "                target = target.to(torch.float32)\n",
    "                #print(torch.tensor(output1[0][0]),torch.tensor(float(labels[i]['multi-author'])))\n",
    "                print(output1,target)\n",
    "                #loss_x1 = loss_function(torch.tensor(output1[0][0]), torch.tensor(float(labels[i]['multi-author'])))\n",
    "                loss_x1 = loss_function(output1[0],target)\n",
    "                evaluation_list.append(([output1,torch.tensor(labels[i]), loss_x1]))\n",
    "                print(i, '  ist=', output1, ' soll=', torch.tensor(labels[i]), ' Loss=', loss_x1)\n",
    "                loss_x1.backward()\n",
    "                optimizer.step()\n",
    "                #if index % log_interval == 0:\n",
    "                 #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epochs, index * len(features),\n",
    "                  #                                                                 len(features),\n",
    "                   #                                                                100. * index / len(features),\n",
    "                    #                                                               loss_x1.data.item()))\n",
    "        return evaluation_list\n",
    "        \n",
    "    def train_x2(self, features, labels, loss_function, optimizer, epochs):\n",
    "        log_interval = 5\n",
    "        evaluation_list = list()\n",
    "        for epoch in range(epochs):\n",
    "            for i, data in enumerate(features):\n",
    "                data = Data_Preprocessing.preprocessing(data)\n",
    "                optimizer.zero_grad()\n",
    "                output1 = self.forward(data)\n",
    "                target =  torch.tensor(labels[i])\n",
    "                target = target.to(torch.float32)\n",
    "                #print(torch.tensor(output1[0][0]),torch.tensor(float(labels[i]['multi-author'])))\n",
    "                print(output1,target)\n",
    "                #loss_x1 = loss_function(torch.tensor(output1[0][0]), torch.tensor(float(labels[i]['multi-author'])))\n",
    "                loss_x1 = loss_function(output1[0],target)\n",
    "                evaluation_list.append(([output1,torch.tensor(labels[i]), loss_x1]))\n",
    "                print(i, '  ist=', output1, ' soll=', torch.tensor(labels[i]), ' Loss=', loss_x1)\n",
    "                loss_x1.backward()\n",
    "                optimizer.step()\n",
    "                #if index % log_interval == 0:\n",
    "                 #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epochs, index * len(features),\n",
    "                  #                                                                 len(features),\n",
    "                   #                                                                100. * index / len(features),\n",
    "                    #                                                               loss_x1.data.item()))\n",
    "        return evaluation_list\n",
    "\n",
    "    def test_model(self, features, labels, loss_function):\n",
    "        log_interval = 5\n",
    "        evaluation_list = list()\n",
    "\n",
    "        for index, data in enumerate(features):\n",
    "            output = self(data)\n",
    "            loss = loss_function(output, labels[index]['multi-author'])\n",
    "            evaluation_list.append(([output,torch.tensor(labels[index]['multi-author']), loss]))\n",
    "            if index % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(index, index * len(data),\n",
    "                                                                               len(data),\n",
    "                                                                               100. * index / len(data),\n",
    "                                                                               loss.data.item()))\n",
    "        return evaluation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/alexbeetz/opt/anaconda3/envs/DL/lib/python3.8/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4496, 0.5105]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "0   ist= tensor([[0.4496, 0.5105]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.7569, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.4539, 0.5049]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "1   ist= tensor([[0.4539, 0.5049]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.7464, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.4635, 0.4944]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "2   ist= tensor([[0.4635, 0.4944]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.7255, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.4746, 0.4833]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "3   ist= tensor([[0.4746, 0.4833]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.7028, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.4904, 0.4665]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "4   ist= tensor([[0.4904, 0.4665]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.6704, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.5107, 0.4473]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "5   ist= tensor([[0.5107, 0.4473]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.6324, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.5266, 0.4299]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "6   ist= tensor([[0.5266, 0.4299]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.6016, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.5446, 0.4114]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "7   ist= tensor([[0.5446, 0.4114]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.5688, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.5623, 0.3933]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "8   ist= tensor([[0.5623, 0.3933]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.5377, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.5795, 0.3777]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "9   ist= tensor([[0.5795, 0.3777]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.5100, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.5958, 0.3610]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "10   ist= tensor([[0.5958, 0.3610]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.4829, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.6105, 0.3485]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "11   ist= tensor([[0.6105, 0.3485]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.4610, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.6234, 0.3352]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "12   ist= tensor([[0.6234, 0.3352]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.4404, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.6370, 0.3227]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "13   ist= tensor([[0.6370, 0.3227]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.4203, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.6499, 0.3105]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "14   ist= tensor([[0.6499, 0.3105]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.4014, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.6611, 0.2997]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "15   ist= tensor([[0.6611, 0.2997]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.3851, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.6733, 0.2893]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "16   ist= tensor([[0.6733, 0.2893]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.3686, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.6842, 0.2783]], grad_fn=<SigmoidBackward0>) tensor([1., 0.])\n",
      "17   ist= tensor([[0.6842, 0.2783]], grad_fn=<SigmoidBackward0>)  soll= tensor([1, 0])  Loss= tensor(0.3528, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb Zelle 9\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001B[0m criterion \u001B[39m=\u001B[39m nn\u001B[39m.\u001B[39mBCELoss()\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m model\u001B[39m.\u001B[39mtrain()\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001B[0m evaluation \u001B[39m=\u001B[39m model\u001B[39m.\u001B[39;49mtrain_x2(features\u001B[39m=\u001B[39;49mtrain_features_new,labels\u001B[39m=\u001B[39;49mtrain_labels_new ,loss_function\u001B[39m=\u001B[39;49mcriterion,optimizer\u001B[39m=\u001B[39;49moptimizer,epochs\u001B[39m=\u001B[39;49m\u001B[39m1\u001B[39;49m)\n",
      "\u001B[1;32m/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb Zelle 9\u001B[0m in \u001B[0;36mNet.train_x2\u001B[0;34m(self, features, labels, loss_function, optimizer, epochs)\u001B[0m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#X11sZmlsZQ%3D%3D?line=59'>60</a>\u001B[0m \u001B[39mfor\u001B[39;00m epoch \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(epochs):\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#X11sZmlsZQ%3D%3D?line=60'>61</a>\u001B[0m     \u001B[39mfor\u001B[39;00m i, data \u001B[39min\u001B[39;00m \u001B[39menumerate\u001B[39m(features):\n\u001B[0;32m---> <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#X11sZmlsZQ%3D%3D?line=61'>62</a>\u001B[0m         data \u001B[39m=\u001B[39m Data_Preprocessing\u001B[39m.\u001B[39;49mpreprocessing(data)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#X11sZmlsZQ%3D%3D?line=62'>63</a>\u001B[0m         optimizer\u001B[39m.\u001B[39mzero_grad()\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#X11sZmlsZQ%3D%3D?line=63'>64</a>\u001B[0m         output1 \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mforward(data)\n",
      "File \u001B[0;32m~/Plagiarism_Detection/Plagiarism_Detection/Data_Preprocessing.py:4\u001B[0m, in \u001B[0;36mpreprocessing\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mpreprocessing\u001B[39m(data):\n\u001B[1;32m      2\u001B[0m     \u001B[39mfrom\u001B[39;00m \u001B[39mtransformers\u001B[39;00m \u001B[39mimport\u001B[39;00m BertTokenizer, BertModel\n\u001B[0;32m----> 4\u001B[0m     tokenizer \u001B[39m=\u001B[39m BertTokenizer\u001B[39m.\u001B[39;49mfrom_pretrained(\u001B[39m'\u001B[39;49m\u001B[39mbert-base-uncased\u001B[39;49m\u001B[39m'\u001B[39;49m)\n\u001B[1;32m      5\u001B[0m     encoded_input \u001B[39m=\u001B[39m tokenizer(data, return_tensors\u001B[39m=\u001B[39m\u001B[39m'\u001B[39m\u001B[39mpt\u001B[39m\u001B[39m'\u001B[39m, truncation\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m, padding\u001B[39m=\u001B[39m\u001B[39m'\u001B[39m\u001B[39mmax_length\u001B[39m\u001B[39m'\u001B[39m, max_length\u001B[39m=\u001B[39m\u001B[39m512\u001B[39m)\n\u001B[1;32m      7\u001B[0m     \u001B[39mreturn\u001B[39;00m encoded_input\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1719\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1717\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1718\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m-> 1719\u001B[0m         resolved_vocab_files[file_id] \u001B[39m=\u001B[39m cached_path(\n\u001B[1;32m   1720\u001B[0m             file_path,\n\u001B[1;32m   1721\u001B[0m             cache_dir\u001B[39m=\u001B[39;49mcache_dir,\n\u001B[1;32m   1722\u001B[0m             force_download\u001B[39m=\u001B[39;49mforce_download,\n\u001B[1;32m   1723\u001B[0m             proxies\u001B[39m=\u001B[39;49mproxies,\n\u001B[1;32m   1724\u001B[0m             resume_download\u001B[39m=\u001B[39;49mresume_download,\n\u001B[1;32m   1725\u001B[0m             local_files_only\u001B[39m=\u001B[39;49mlocal_files_only,\n\u001B[1;32m   1726\u001B[0m             use_auth_token\u001B[39m=\u001B[39;49muse_auth_token,\n\u001B[1;32m   1727\u001B[0m             user_agent\u001B[39m=\u001B[39;49muser_agent,\n\u001B[1;32m   1728\u001B[0m         )\n\u001B[1;32m   1730\u001B[0m     \u001B[39mexcept\u001B[39;00m \u001B[39mFileNotFoundError\u001B[39;00m \u001B[39mas\u001B[39;00m error:\n\u001B[1;32m   1731\u001B[0m         \u001B[39mif\u001B[39;00m local_files_only:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/transformers/utils/hub.py:282\u001B[0m, in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m    278\u001B[0m     local_files_only \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n\u001B[1;32m    280\u001B[0m \u001B[39mif\u001B[39;00m is_remote_url(url_or_filename):\n\u001B[1;32m    281\u001B[0m     \u001B[39m# URL, so get it from the cache (downloading if necessary)\u001B[39;00m\n\u001B[0;32m--> 282\u001B[0m     output_path \u001B[39m=\u001B[39m get_from_cache(\n\u001B[1;32m    283\u001B[0m         url_or_filename,\n\u001B[1;32m    284\u001B[0m         cache_dir\u001B[39m=\u001B[39;49mcache_dir,\n\u001B[1;32m    285\u001B[0m         force_download\u001B[39m=\u001B[39;49mforce_download,\n\u001B[1;32m    286\u001B[0m         proxies\u001B[39m=\u001B[39;49mproxies,\n\u001B[1;32m    287\u001B[0m         resume_download\u001B[39m=\u001B[39;49mresume_download,\n\u001B[1;32m    288\u001B[0m         user_agent\u001B[39m=\u001B[39;49muser_agent,\n\u001B[1;32m    289\u001B[0m         use_auth_token\u001B[39m=\u001B[39;49muse_auth_token,\n\u001B[1;32m    290\u001B[0m         local_files_only\u001B[39m=\u001B[39;49mlocal_files_only,\n\u001B[1;32m    291\u001B[0m     )\n\u001B[1;32m    292\u001B[0m \u001B[39melif\u001B[39;00m os\u001B[39m.\u001B[39mpath\u001B[39m.\u001B[39mexists(url_or_filename):\n\u001B[1;32m    293\u001B[0m     \u001B[39m# File, and it exists.\u001B[39;00m\n\u001B[1;32m    294\u001B[0m     output_path \u001B[39m=\u001B[39m url_or_filename\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/transformers/utils/hub.py:485\u001B[0m, in \u001B[0;36mget_from_cache\u001B[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m    483\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m local_files_only:\n\u001B[1;32m    484\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 485\u001B[0m         r \u001B[39m=\u001B[39m requests\u001B[39m.\u001B[39;49mhead(url, headers\u001B[39m=\u001B[39;49mheaders, allow_redirects\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m, proxies\u001B[39m=\u001B[39;49mproxies, timeout\u001B[39m=\u001B[39;49metag_timeout)\n\u001B[1;32m    486\u001B[0m         _raise_for_status(r)\n\u001B[1;32m    487\u001B[0m         etag \u001B[39m=\u001B[39m r\u001B[39m.\u001B[39mheaders\u001B[39m.\u001B[39mget(\u001B[39m\"\u001B[39m\u001B[39mX-Linked-Etag\u001B[39m\u001B[39m\"\u001B[39m) \u001B[39mor\u001B[39;00m r\u001B[39m.\u001B[39mheaders\u001B[39m.\u001B[39mget(\u001B[39m\"\u001B[39m\u001B[39mETag\u001B[39m\u001B[39m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/requests/api.py:102\u001B[0m, in \u001B[0;36mhead\u001B[0;34m(url, **kwargs)\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[39mr\u001B[39m\u001B[39m\"\"\"Sends a HEAD request.\u001B[39;00m\n\u001B[1;32m     92\u001B[0m \n\u001B[1;32m     93\u001B[0m \u001B[39m:param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[39m:rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m    101\u001B[0m kwargs\u001B[39m.\u001B[39msetdefault(\u001B[39m'\u001B[39m\u001B[39mallow_redirects\u001B[39m\u001B[39m'\u001B[39m, \u001B[39mFalse\u001B[39;00m)\n\u001B[0;32m--> 102\u001B[0m \u001B[39mreturn\u001B[39;00m request(\u001B[39m'\u001B[39;49m\u001B[39mhead\u001B[39;49m\u001B[39m'\u001B[39;49m, url, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/requests/api.py:61\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[39m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[39m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[39mwith\u001B[39;00m sessions\u001B[39m.\u001B[39mSession() \u001B[39mas\u001B[39;00m session:\n\u001B[0;32m---> 61\u001B[0m     \u001B[39mreturn\u001B[39;00m session\u001B[39m.\u001B[39;49mrequest(method\u001B[39m=\u001B[39;49mmethod, url\u001B[39m=\u001B[39;49murl, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/requests/sessions.py:529\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    524\u001B[0m send_kwargs \u001B[39m=\u001B[39m {\n\u001B[1;32m    525\u001B[0m     \u001B[39m'\u001B[39m\u001B[39mtimeout\u001B[39m\u001B[39m'\u001B[39m: timeout,\n\u001B[1;32m    526\u001B[0m     \u001B[39m'\u001B[39m\u001B[39mallow_redirects\u001B[39m\u001B[39m'\u001B[39m: allow_redirects,\n\u001B[1;32m    527\u001B[0m }\n\u001B[1;32m    528\u001B[0m send_kwargs\u001B[39m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 529\u001B[0m resp \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49msend(prep, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49msend_kwargs)\n\u001B[1;32m    531\u001B[0m \u001B[39mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/requests/sessions.py:645\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    642\u001B[0m start \u001B[39m=\u001B[39m preferred_clock()\n\u001B[1;32m    644\u001B[0m \u001B[39m# Send the request\u001B[39;00m\n\u001B[0;32m--> 645\u001B[0m r \u001B[39m=\u001B[39m adapter\u001B[39m.\u001B[39;49msend(request, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m    647\u001B[0m \u001B[39m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    648\u001B[0m elapsed \u001B[39m=\u001B[39m preferred_clock() \u001B[39m-\u001B[39m start\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/requests/adapters.py:440\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    438\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m    439\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m chunked:\n\u001B[0;32m--> 440\u001B[0m         resp \u001B[39m=\u001B[39m conn\u001B[39m.\u001B[39;49murlopen(\n\u001B[1;32m    441\u001B[0m             method\u001B[39m=\u001B[39;49mrequest\u001B[39m.\u001B[39;49mmethod,\n\u001B[1;32m    442\u001B[0m             url\u001B[39m=\u001B[39;49murl,\n\u001B[1;32m    443\u001B[0m             body\u001B[39m=\u001B[39;49mrequest\u001B[39m.\u001B[39;49mbody,\n\u001B[1;32m    444\u001B[0m             headers\u001B[39m=\u001B[39;49mrequest\u001B[39m.\u001B[39;49mheaders,\n\u001B[1;32m    445\u001B[0m             redirect\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m,\n\u001B[1;32m    446\u001B[0m             assert_same_host\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m,\n\u001B[1;32m    447\u001B[0m             preload_content\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m,\n\u001B[1;32m    448\u001B[0m             decode_content\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m,\n\u001B[1;32m    449\u001B[0m             retries\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmax_retries,\n\u001B[1;32m    450\u001B[0m             timeout\u001B[39m=\u001B[39;49mtimeout\n\u001B[1;32m    451\u001B[0m         )\n\u001B[1;32m    453\u001B[0m     \u001B[39m# Send the request.\u001B[39;00m\n\u001B[1;32m    454\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    455\u001B[0m         \u001B[39mif\u001B[39;00m \u001B[39mhasattr\u001B[39m(conn, \u001B[39m'\u001B[39m\u001B[39mproxy_pool\u001B[39m\u001B[39m'\u001B[39m):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    700\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_prepare_proxy(conn)\n\u001B[1;32m    702\u001B[0m \u001B[39m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m httplib_response \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_make_request(\n\u001B[1;32m    704\u001B[0m     conn,\n\u001B[1;32m    705\u001B[0m     method,\n\u001B[1;32m    706\u001B[0m     url,\n\u001B[1;32m    707\u001B[0m     timeout\u001B[39m=\u001B[39;49mtimeout_obj,\n\u001B[1;32m    708\u001B[0m     body\u001B[39m=\u001B[39;49mbody,\n\u001B[1;32m    709\u001B[0m     headers\u001B[39m=\u001B[39;49mheaders,\n\u001B[1;32m    710\u001B[0m     chunked\u001B[39m=\u001B[39;49mchunked,\n\u001B[1;32m    711\u001B[0m )\n\u001B[1;32m    713\u001B[0m \u001B[39m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[1;32m    714\u001B[0m \u001B[39m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[1;32m    715\u001B[0m \u001B[39m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[1;32m    716\u001B[0m \u001B[39m# mess.\u001B[39;00m\n\u001B[1;32m    717\u001B[0m response_conn \u001B[39m=\u001B[39m conn \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m release_conn \u001B[39melse\u001B[39;00m \u001B[39mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/urllib3/connectionpool.py:449\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    444\u001B[0m             httplib_response \u001B[39m=\u001B[39m conn\u001B[39m.\u001B[39mgetresponse()\n\u001B[1;32m    445\u001B[0m         \u001B[39mexcept\u001B[39;00m \u001B[39mBaseException\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    446\u001B[0m             \u001B[39m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[1;32m    447\u001B[0m             \u001B[39m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[1;32m    448\u001B[0m             \u001B[39m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[0;32m--> 449\u001B[0m             six\u001B[39m.\u001B[39;49mraise_from(e, \u001B[39mNone\u001B[39;49;00m)\n\u001B[1;32m    450\u001B[0m \u001B[39mexcept\u001B[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    451\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_raise_timeout(err\u001B[39m=\u001B[39me, url\u001B[39m=\u001B[39murl, timeout_value\u001B[39m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[0;34m(value, from_value)\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/site-packages/urllib3/connectionpool.py:444\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    441\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mTypeError\u001B[39;00m:\n\u001B[1;32m    442\u001B[0m     \u001B[39m# Python 3\u001B[39;00m\n\u001B[1;32m    443\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 444\u001B[0m         httplib_response \u001B[39m=\u001B[39m conn\u001B[39m.\u001B[39;49mgetresponse()\n\u001B[1;32m    445\u001B[0m     \u001B[39mexcept\u001B[39;00m \u001B[39mBaseException\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    446\u001B[0m         \u001B[39m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[1;32m    447\u001B[0m         \u001B[39m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[1;32m    448\u001B[0m         \u001B[39m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[1;32m    449\u001B[0m         six\u001B[39m.\u001B[39mraise_from(e, \u001B[39mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/http/client.py:1348\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1346\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m   1347\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m-> 1348\u001B[0m         response\u001B[39m.\u001B[39;49mbegin()\n\u001B[1;32m   1349\u001B[0m     \u001B[39mexcept\u001B[39;00m \u001B[39mConnectionError\u001B[39;00m:\n\u001B[1;32m   1350\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/http/client.py:316\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[39m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    315\u001B[0m \u001B[39mwhile\u001B[39;00m \u001B[39mTrue\u001B[39;00m:\n\u001B[0;32m--> 316\u001B[0m     version, status, reason \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_read_status()\n\u001B[1;32m    317\u001B[0m     \u001B[39mif\u001B[39;00m status \u001B[39m!=\u001B[39m CONTINUE:\n\u001B[1;32m    318\u001B[0m         \u001B[39mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/http/client.py:277\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_read_status\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[0;32m--> 277\u001B[0m     line \u001B[39m=\u001B[39m \u001B[39mstr\u001B[39m(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mfp\u001B[39m.\u001B[39;49mreadline(_MAXLINE \u001B[39m+\u001B[39;49m \u001B[39m1\u001B[39;49m), \u001B[39m\"\u001B[39m\u001B[39miso-8859-1\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m    278\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mlen\u001B[39m(line) \u001B[39m>\u001B[39m _MAXLINE:\n\u001B[1;32m    279\u001B[0m         \u001B[39mraise\u001B[39;00m LineTooLong(\u001B[39m\"\u001B[39m\u001B[39mstatus line\u001B[39m\u001B[39m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/socket.py:669\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    667\u001B[0m \u001B[39mwhile\u001B[39;00m \u001B[39mTrue\u001B[39;00m:\n\u001B[1;32m    668\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 669\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_sock\u001B[39m.\u001B[39;49mrecv_into(b)\n\u001B[1;32m    670\u001B[0m     \u001B[39mexcept\u001B[39;00m timeout:\n\u001B[1;32m    671\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_timeout_occurred \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/ssl.py:1241\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1237\u001B[0m     \u001B[39mif\u001B[39;00m flags \u001B[39m!=\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[1;32m   1238\u001B[0m         \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\n\u001B[1;32m   1239\u001B[0m           \u001B[39m\"\u001B[39m\u001B[39mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m\"\u001B[39m \u001B[39m%\u001B[39m\n\u001B[1;32m   1240\u001B[0m           \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m\u001B[39m__class__\u001B[39m)\n\u001B[0;32m-> 1241\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mread(nbytes, buffer)\n\u001B[1;32m   1242\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1243\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39msuper\u001B[39m()\u001B[39m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DL/lib/python3.8/ssl.py:1099\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1097\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m   1098\u001B[0m     \u001B[39mif\u001B[39;00m buffer \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m-> 1099\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_sslobj\u001B[39m.\u001B[39;49mread(\u001B[39mlen\u001B[39;49m, buffer)\n\u001B[1;32m   1100\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1101\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_sslobj\u001B[39m.\u001B[39mread(\u001B[39mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "criterion = nn.BCELoss()\n",
    "model.train()\n",
    "evaluation = model.train_x2(features=train_features_new,labels=train_labels_new ,loss_function=criterion,optimizer=optimizer,epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ea37bf44d4b176858fce52ed9f8f080b9ac5c60af0816acde5b07bb26db502c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
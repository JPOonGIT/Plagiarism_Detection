{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import Doc_import\n",
    "import os\n",
    "import json\n",
    "import Data_Preprocessing\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_train_labels = '/Users/alexbeetz/pan20-authorship-verification-training-small/pan20-authorship-verification-training-small-truth.jsonl'\n",
    "fname_train_features = '/Users/alexbeetz/pan20-authorship-verification-training-small/pan20-authorship-verification-training-small.jsonl'\n",
    "\n",
    "same_author, texts = Doc_import.text_preprocessing(fname_train_labels, fname_train_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_task2_binary(same_author):\n",
    "    new_labels = []\n",
    "    binary = []\n",
    "    count_ja = 0\n",
    "    count_nein = 0\n",
    "    values = list(same_author.values())\n",
    "    for label in values:\n",
    "        if label == 1:\n",
    "            binary.append(1)\n",
    "            new_labels.append([1,0])\n",
    "            count_ja += 1\n",
    "        else:\n",
    "            binary.append(0)\n",
    "            new_labels.append([1,0])\n",
    "            count_nein += 1\n",
    "    return new_labels, binary, count_ja, count_nein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,bin, ja, nein = preprocessing_task2_binary(same_author)\n",
    "#print(ja, nein)\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja = list()\n",
    "nein = list()\n",
    "for i, b in enumerate(bin):\n",
    "    if b == 1:\n",
    "        ja.append(i)\n",
    "    else: nein.append(i)\n",
    "ja = ja[:2800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(ja)\n",
    "random.shuffle(nein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_new = list()\n",
    "train_features_new = list()\n",
    "\n",
    "for i in range(2800):\n",
    "    train_labels_new.append(labels[nein[i]])\n",
    "    train_labels_new.append(labels[ja[i]])\n",
    "    train_features_new.append(texts[nein[i]])\n",
    "    train_features_new.append(texts[ja[i]])\n",
    "#train_labels_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import Data_Preprocessing\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        kernel = 4\n",
    "        input_ff = int(393216/(kernel * kernel))\n",
    "        zwischenlayer1 = int(1000)\n",
    "        zwischenlayer2 = int(10)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.pooling = nn.MaxPool2d(kernel)\n",
    "        self.linear11 = nn.Linear(input_ff, zwischenlayer1)\n",
    "        self.linear12 = nn.Linear(zwischenlayer1, zwischenlayer2)\n",
    "        self.linear13 = nn.Linear(zwischenlayer2, 2)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        bert_output = self.bert(**input_data)\n",
    "        bert_output = self.pooling(bert_output[0])\n",
    "        bert_output = bert_output[0].view(-1, 24576)\n",
    "        x1 = torch.softmax(self.linear11(bert_output))\n",
    "        x1 = torch.softmax(self.linear12(x1))\n",
    "        x1 = torch.softmax(self.linear13(x1))\n",
    "\n",
    "        return x1\n",
    "\n",
    "    def train_x1(self, features, labels, loss_function, optimizer, epochs):\n",
    "        log_interval = 5\n",
    "        evaluation_list = list()\n",
    "        for epoch in range(epochs):\n",
    "            for i, data in enumerate(features):\n",
    "                data = Data_Preprocessing.preprocessing(data)\n",
    "                optimizer.zero_grad()\n",
    "                output1 = self.forward(data)\n",
    "                target =  torch.tensor(labels[i])\n",
    "                target = target.to(torch.float32)\n",
    "                #print(torch.tensor(output1[0][0]),torch.tensor(float(labels[i]['multi-author'])))\n",
    "                print(output1,target)\n",
    "                #loss_x1 = loss_function(torch.tensor(output1[0][0]), torch.tensor(float(labels[i]['multi-author'])))\n",
    "                loss_x1 = loss_function(output1[0],target)\n",
    "                evaluation_list.append(([output1,torch.tensor(labels[i]), loss_x1]))\n",
    "                print(i, '  ist=', output1, ' soll=', torch.tensor(labels[i]), ' Loss=', loss_x1)\n",
    "                loss_x1.backward()\n",
    "                optimizer.step()\n",
    "                #if index % log_interval == 0:\n",
    "                 #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epochs, index * len(features),\n",
    "                  #                                                                 len(features),\n",
    "                   #                                                                100. * index / len(features),\n",
    "                    #                                                               loss_x1.data.item()))\n",
    "        return evaluation_list\n",
    "        \n",
    "    def train_x2(self, features, labels, loss_function, optimizer, epochs):\n",
    "        log_interval = 5\n",
    "        evaluation_list = list()\n",
    "        for epoch in range(epochs):\n",
    "            for i, data in enumerate(features):\n",
    "                data = Data_Preprocessing.preprocessing(data)\n",
    "                optimizer.zero_grad()\n",
    "                output1 = self.forward(data)\n",
    "                target =  torch.tensor(labels[i])\n",
    "                target = target.to(torch.float32)\n",
    "                #print(torch.tensor(output1[0][0]),torch.tensor(float(labels[i]['multi-author'])))\n",
    "                print(output1,target)\n",
    "                #loss_x1 = loss_function(torch.tensor(output1[0][0]), torch.tensor(float(labels[i]['multi-author'])))\n",
    "                loss_x1 = loss_function(output1[0],target)\n",
    "                evaluation_list.append(([output1,torch.tensor(labels[i]), loss_x1]))\n",
    "                print(i, '  ist=', output1, ' soll=', torch.tensor(labels[i]), ' Loss=', loss_x1)\n",
    "                loss_x1.backward()\n",
    "                optimizer.step()\n",
    "                #if index % log_interval == 0:\n",
    "                 #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epochs, index * len(features),\n",
    "                  #                                                                 len(features),\n",
    "                   #                                                                100. * index / len(features),\n",
    "                    #                                                               loss_x1.data.item()))\n",
    "        return evaluation_list\n",
    "\n",
    "    def test_model(self, features, labels, loss_function):\n",
    "        log_interval = 5\n",
    "        evaluation_list = list()\n",
    "\n",
    "        for index, data in enumerate(features):\n",
    "            output = self(data)\n",
    "            loss = loss_function(output, labels[index]['multi-author'])\n",
    "            evaluation_list.append(([output,torch.tensor(labels[index]['multi-author']), loss]))\n",
    "            if index % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(index, index * len(data),\n",
    "                                                                               len(data),\n",
    "                                                                               100. * index / len(data),\n",
    "                                                                               loss.data.item()))\n",
    "        return evaluation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype)\n * (Tensor input, name dim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb Zelle 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mL1Loss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain_x2(features\u001b[39m=\u001b[39;49mtrain_features_new,labels\u001b[39m=\u001b[39;49mtrain_labels_new ,loss_function\u001b[39m=\u001b[39;49mcriterion,optimizer\u001b[39m=\u001b[39;49moptimizer,epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb Zelle 9\u001b[0m in \u001b[0;36mNet.train_x2\u001b[0;34m(self, features, labels, loss_function, optimizer, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m data \u001b[39m=\u001b[39m Data_Preprocessing\u001b[39m.\u001b[39mpreprocessing(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m output1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m target \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39mtensor(labels[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[1;32m/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb Zelle 9\u001b[0m in \u001b[0;36mNet.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m bert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooling(bert_output[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m bert_output \u001b[39m=\u001b[39m bert_output[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m24576\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m x1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msoftmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear11(bert_output))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m x1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear12(x1))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexbeetz/Plagiarism_Detection/Plagiarism_Detection/bert_second_head.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m x1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear13(x1))\n",
      "\u001b[0;31mTypeError\u001b[0m: softmax() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype)\n * (Tensor input, name dim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "criterion = nn.L1Loss()\n",
    "model.train()\n",
    "model.train_x2(features=train_features_new,labels=train_labels_new ,loss_function=criterion,optimizer=optimizer,epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ea37bf44d4b176858fce52ed9f8f080b9ac5c60af0816acde5b07bb26db502c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
